# -*- coding: utf-8 -*-
"""CIS 519 Project Part 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H0P7x6oEao-kN7XoYBymK4UYl-pK4raR

#**CIS 5190 Final Project**

By: Ethan Ma, Eric Wang, Chung Un Lee (Richard)

# Sports Betting with ML
### Contribution 1:
In this first contribution, we will focus on collecting data and then using it to create our baseline models.

## Installations, Imports, and Set-Up:
"""

!pip install pandasql

import csv
import pandas as pd
import numpy as np
import datetime as dt
import geopy.distance as gp
import matplotlib.image as mpimg
import plotly.express as px
import pandasql as ps #SQL on Pandas Dataframe
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

"""Mount to Drive (need to sign-in to Google Account)

### Datasets

[Link to Drive](https://drive.google.com/drive/folders/1PdNkjqxJQytu9w2NpsDEIv6e0gmn1ZaZ?usp=sharing)

To run:
1: Download 23_players_reg_pergame, 23_win_loss_data, 23_players_reg_pergame as .csv files to your local Google Drive
"""

# Mount to google drive
from google.colab import drive
drive.mount('/content/drive')

"""## Data Import, Cleaning, and Processing

Load the dataset using Pandas.
Please download all .csv files found in the 'Google Colab Data' folder of this drive:
https://drive.google.com/drive/folders/1PdNkjqxJQytu9w2NpsDEIv6e0gmn1ZaZ?usp=sharing
"""

# Initialize news_df using our downloaded csv
win_loss_df = pd.read_csv('/content/drive/MyDrive/23_win_loss_data.csv')
team_df = pd.read_csv('/content/drive/MyDrive/23_team_reg_pergame.csv')
player_df = pd.read_csv('/content/drive/MyDrive/23_players_reg_pergame.csv')
line_df = pd.read_csv('/content/drive/MyDrive/ROTOWIRE_data.csv')

# Show the first 10 rows of news_df
win_loss_df

team_df.head(10)

player_df.head(10)

"""Drop null values, any cleaning here"""

# TODO

"""# Cleaning Data and Creating Datasets

Import nltk modules

Create new column in ```news_df``` called **'cleaned_headlines'** that holds the cleaned headlines. To clean headlines, break each headline into tokens, have each token converted to lower case, only consider works that contain only letters, and filter out any stop words.
"""

# Define function to change each team to its 3 letter abreviation
def convert_team(team):
  mapping = {
      'Hawks': 'ATL',
      'Celtics': 'BOS',
      'Nets': 'BKN',
      'Hornets': 'CHA',
      'Bulls': 'CHI',
      'Cavaliers': 'CLE',
      'Mavericks': 'DAL',
      'Nuggets': 'DEN',
      'Pistons': 'DET',
      'Warriors': 'GSW',
      'Rockets': 'HOU',
      'Pacers': 'IND',
      'Clippers': 'LAC',
      'Lakers': 'LAL',
      'Grizzlies': 'MEM',
      'Heat': 'MIA',
      'Bucks': 'MIL',
      'Timberwolves': 'MIN',
      'Pelicans': 'NOP',
      'Knicks': 'NYK',
      'Thunder': 'OKC',
      'Magic': 'ORL',
      '76ers': 'PHI',
      'Suns': 'PHX',
      'Blazers': 'POR',
      'Kings': 'SAC',
      'Spurs': 'SAS',
      'Raptors': 'TOR',
      'Jazz': 'UTA',
      'Wizards': 'WAS'
  }
  # remove regional location for team, only want the name
  if " " in team:
    team_name = team.split(' ')
    team = team_name[-1]
  return mapping[team] if team in mapping else team

"""Clean each dataset to have the same 3-letter team name format


"""

# Apply above function to each row to convert team name
cleaned_win_loss_df = win_loss_df.astype({"home_team": "string", "away_team": "string", "winner": "string", "loser": "string"})

cleaned_win_loss_df['home_team'] = cleaned_win_loss_df['home_team'].apply(lambda x: convert_team(x))
cleaned_win_loss_df['away_team'] = cleaned_win_loss_df['away_team'].apply(lambda x: convert_team(x))
cleaned_win_loss_df['winner'] = cleaned_win_loss_df['winner'].apply(lambda x: convert_team(x))
cleaned_win_loss_df['loser'] = cleaned_win_loss_df['loser'].apply(lambda x: convert_team(x))

# create label, 1 if the home team won, 0 otherwise
def create_label(row):
  return 1 if row['home_team'] == row['winner'] else 0
cleaned_win_loss_df['label'] = cleaned_win_loss_df.apply(create_label, axis=1)


cleaned_win_loss_df.head(10)

cleaned_team_df = team_df.astype({"TEAM": "string"})

cleaned_team_df['TEAM'] = cleaned_team_df['TEAM'].apply(lambda x: convert_team(x))
cleaned_team_df = cleaned_team_df.rename(columns={"Team": "TEAM", "3PM": "TPM", "3PA": "TPA"})
cleaned_team_df.head(10)

cleaned_player_df = player_df.astype({"Team": "string"})
cleaned_player_df = cleaned_player_df.rename(columns={"Team": "TEAM", "3PM": "TPM", "3PA": "TPA"})
cleaned_player_df.head(10)

cleaned_player_df = cleaned_player_df.drop(columns=['Player'])
cleaned_player_df

# create label, 1 if the home team won, 0 otherwise
def create_label(row):
  return 1 if row['home_score'] > row['away_score'] else 0

cleaned_line_df = line_df
cleaned_line_df['label'] = cleaned_line_df.apply(create_label, axis=1)

# let's only consider 2023 data for now
cleaned_line_df = cleaned_line_df[cleaned_line_df['season'] == 2022]
# again get rid of scores, since this would not be available until after the game is over
# SHOULD NOT TAKE LABEL OFF, THINK ABT THIS MORE

# List of columns to keep for aggregation
columns_to_keep = ['MIN', 'PTS', 'FGM', 'FGA', 'FG%', 'TPM', 'TPA', '3P%',
                   'FTM', 'FTA', 'FT%', 'OREB', 'DREB', 'REB', 'AST', 'STL',
                   'BLK', 'TOV', 'EFF']

sorted_df = cleaned_player_df.sort_values(by=['TEAM', 'Rank'], ascending=[True, False])

# Group by 'TEAM' and take the top 3 players for each team
top_3_players = sorted_df.groupby('TEAM').head(3)

# Group the top 3 players DataFrame by 'TEAM' and calculate the mean for the specified columns
player_avg_df = top_3_players.groupby('TEAM')[columns_to_keep].mean()


new_column_names = ['p_' + col for col in columns_to_keep]

# Rename the columns in the DataFrame
player_avg_df.columns = new_column_names


# Print the aggregated DataFrame
player_avg_df.head(10)

"""Create joined dataframe

Drop Unecessary/Redundant Columns
"""

cleaned_win_loss_df = cleaned_win_loss_df.drop(columns=['status', 'winner', 'loser', 'home_score', 'away_score']) # scores won't be available when predicting games before actual matchup
cleaned_win_loss_df

# cross join all team stats
cross_query = """
SELECT T1.TEAM AS home_team, T2.TEAM AS away_team, T1.W - T2.w AS W, T1.PTS - T2.PTS AS PTS, T1.FGM - T2.FGM AS FGM, T1.FGA - T2.FGA AS FGA, T1.TPA - T2.TPA AS TPA, T1.TPM - T2.TPM AS TPM, T1.FTM - T2.FTM AS FTM, T1.FTA - T2.FTA AS FTA, T1.OREB - T2.OREB AS OREB, T1.DREB - T2.DREB AS DREB, T1.REB - T2.REB AS REB, T1.AST - T2.AST AS AST, T1.TOV - T2.TOV AS TOV, T1.STL - T2.STL AS STL, T1.BLK - T2.BLK AS BLK, T1.BLKA - T2.BLKA AS BLKA, T1.PF - T2.PF AS PF, T1.PFD - T2.PFD AS PFD
FROM cleaned_team_df T1 CROSS JOIN cleaned_team_df T2
WHERE NOT T1.TEAM=T2.TEAM
"""
cross_df = ps.sqldf(cross_query, locals())
cross_df

# cross all player average stats
player_cross_query = """
SELECT T1.TEAM AS home_team, T2.TEAM AS away_team, T1.p_PTS - T2.p_PTS AS p_PTS, T1.p_FGM - T2.p_FGM AS p_FGM, T1.p_FGA - T2.p_FGA AS p_FGA, T1.p_TPA - T2.p_TPA AS p_TPA, T1.p_TPM - T2.p_TPM AS p_TPM, T1.p_FTM - T2.p_FTM AS p_FTM, T1.p_FTA - T2.p_FTA AS p_FTA, T1.p_OREB - T2.p_OREB AS p_OREB, T1.p_DREB - T2.p_DREB AS p_DREB, T1.p_REB - T2.p_REB AS p_REB, T1.p_AST - T2.p_AST AS p_AST, T1.p_TOV - T2.p_TOV AS p_TOV, T1.p_STL - T2.p_STL AS p_STL, T1.p_BLK - T2.p_BLK AS p_BLK
FROM player_avg_df T1 CROSS JOIN player_avg_df T2
WHERE NOT T1.TEAM=T2.TEAM
"""
player_cross_df = ps.sqldf(player_cross_query, locals())
cross_df

join_query = """
SELECT DISTINCT label, home_line, tipoff, away_score, home_score, W.home_team, W.away_team, W, PTS, FGM, FGA, TPA, TPM, FTM,
       FTA, OREB, DREB, REB, AST, TOV, STL, BLK, BLKA, PF,
       PFD, season
FROM cross_df C JOIN cleaned_line_df W ON C.home_team = W.home_team AND C.away_team = W.away_team

"""
joined_df = ps.sqldf(join_query, locals())
joined_df
print(joined_df.columns)

# join2_query = """
# SELECT DISTINCT W.home_team, W.away_team, W, PTS, FGM, FGA, TPA, TPM, FTM,
#        FTA, OREB, DREB, REB, AST, TOV, STL, BLK, BLKA, PF,
#        PFD, season, date, label
# FROM cross_df C JOIN cleaned_win_loss_df W ON C.home_team = W.home_team AND C.away_team = W.away_team

# """
join2_query = """
SELECT *
FROM player_cross_df C JOIN joined_df W ON C.home_team = W.home_team AND C.away_team = W.away_team

"""
joined2_df = ps.sqldf(join2_query, locals())
joined2_df.head()
# print(joined2_df.columns)

player_query = """
SELECT *
FROM joined_df j JOIN cleaned_player_df p ON j.home_team = p.TEAM
"""
joined_player_df = ps.sqldf(player_query, locals())
joined_player_df

print("Columns in joined_player_df:", joined_player_df.columns)

final_df = joined2_df.drop(columns=['W', 'season', 'home_team', 'away_team'])
# final_df = joined2_df.drop(columns=['W', 'season', 'date', 'home_team', 'away_team', 'TEAM'])
final_df

"""# Creating Test/Training Datasets

First, we use sklearn's ```train_test_split``` to split from the ```cleaned_news_df``` into training and testing sets.
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


# Create features from all except label
features = final_df.drop(columns='label')

# Extract target from the label
target = final_df['label']

# Set seed to 42, then perform the train test split
seed = 42
X_train, X_test, y_train, y_test = train_test_split(features, target, random_state = seed, test_size = 0.2)

# Scale data
# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# x_test = scaler.transform(X_test)

"""## Logistic Regression Model (sklearn)

###Section 3.3.1 The Base Model

WRITE DESCRIPTION HERE
"""

# Import our logistic regression module from sklearn
from sklearn.linear_model import LogisticRegression

# Set seed to 42, then initialize and fit our model
seed = 42
logR = LogisticRegression(max_iter=10000)

logR.fit(X_train, y_train)

from sklearn.metrics import accuracy_score

# With our newly fitted model, predict on both training and testing data
y_pred = logR.predict(X_test)
y_pred_train = logR.predict(X_train)

lr_test_accuracy = accuracy_score(y_pred, y_test)
lr_train_accuracy = accuracy_score(y_pred_train, y_train)
print("Accuracy of train Logistic Classifier: %.1f%%"% (lr_train_accuracy*100))
print("Accuracy of test Logistic Classifier: %.1f%%"% (lr_test_accuracy*100))

logR.predict_proba(X_train)

"""### Analysis of Feature Importances
TODO
"""

from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier


pca = PCA(n_components=len(features.columns))
X_pca = pca.fit_transform(features)

rf_pca = RandomForestClassifier(n_estimators=100,
                                max_depth=6,
                                random_state=seed,
                                class_weight='balanced')
rf_pca.fit(X_pca, target)

importances = rf_pca.feature_importances_

sorted_indices = np.argsort(importances)[::-1]

top_features_indices = sorted_indices[:10]
top_features = features.columns[top_features_indices]

print("Top Features by Importance:")
for feature, importance in zip(top_features, importances[top_features_indices]):
    print(f"{feature}: {importance:.4f}")

import matplotlib.pyplot as plt


component_values = []
test_accuracies = []

for n_components in range(1, 18):
    pca = PCA(n_components=n_components)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)

    seed = 42
    log_pca = LogisticRegression(max_iter=10000)
    log_pca.fit(X_train_pca, y_train)

    y_pred_pca = log_pca.predict(X_test_pca)

    test_accuracy_pca = accuracy_score(y_test, y_pred_pca)

    component_values.append(n_components)
    test_accuracies.append(test_accuracy_pca)

plt.figure(figsize=(10, 6))
plt.plot(component_values, test_accuracies, marker='o')
plt.title('Logistic Regression Accuracy vs. Number of PCA Components')
plt.xlabel('Number of Components')
plt.ylabel('Test Accuracy')
plt.xticks(component_values)
plt.grid(True)
plt.show()

"""## Random Forest Classifier Model

ADD DESCRIPTION
"""

# Import all modules for random forest classifer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

# Setting seed to 42, initialize our Random Forest Classifier
seed = 42
rf = RandomForestClassifier(n_estimators=250,
                            max_depth=2,
                            random_state=seed)

# Fit the classifier on our dataset
rf.fit(X_train, y_train)

# Find accuracies for both training and test data
y_pred = rf.predict(X_test)
y_pred_train = rf.predict(X_train)

train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_pred, y_test)

print("Training Accuracy of Random Forest Classifier: %.1f%%"% (train_accuracy*100))
print("Testing Accuracy of Random Forest Classifier: %.1f%%"% (test_accuracy*100))

"""Using model for different season data"""



"""Save our final dataframe to .csv file"""

from google.colab import files

final_df.to_csv('combinedData.csv', encoding = 'utf-8-sig')
files.download('combinedData.csv')

joined_df.to_csv('combinedDataWithTeams.csv', encoding = 'utf-8-sig')
files.download('combinedDataWithTeams.csv')